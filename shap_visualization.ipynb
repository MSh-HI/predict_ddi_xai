{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import json\n",
    "import torch\n",
    "from src.utils import convert_tensor\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from src.model import build_model\n",
    "import pickle\n",
    "import glob\n",
    "import shap\n",
    "import pandas as pd\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading necessary files and initializing the explainer \n",
    "\n",
    "# Load test input samples\n",
    "SS_mat = pd.read_pickle('./data/structural_similarity_matrix.pkl')\n",
    "TS_mat = pd.read_pickle('./data/target_similarity_matrix.pkl')\n",
    "GS_mat = pd.read_pickle('./data/GO_similarity_matrix.pkl')\n",
    "\n",
    "drugPair2effectIdx = pd.read_pickle('./data/drugPair2effect_idx.pkl')\n",
    "mlb = pd.read_pickle('./data/mlb.pkl')\n",
    "idx2label = pd.read_pickle('./data/idx2label.pkl')\n",
    "\n",
    "data = np.load(\"shap_test_final.npz\")\n",
    "x_test = data[\"x_shap\"]\n",
    "y_test = data[\"y_shap\"]\n",
    "\n",
    "# Check the shape of the slices\n",
    "print(f\"x_test shape: {x_test.shape}\")  \n",
    "print(f\"y_test shape: {y_test.shape}\")  \n",
    "\n",
    "SS_test, TS_test, GS_test, y_test = convert_tensor(x_test, y_test, SS_mat, TS_mat, GS_mat, mlb, idx2label)\n",
    "\n",
    "# Verify the shapes of SS_test, TS_test, GS_test after conversion\n",
    "print(f\"SS_test shape: {SS_test.shape}\")\n",
    "print(f\"TS_test shape: {TS_test.shape}\")\n",
    "print(f\"GS_test shape: {GS_test.shape}\")\n",
    "\n",
    "# Load hyperparameters and data\n",
    "with open('./data/hyperparameter.json') as fp:\n",
    "    hparam = json.load(fp)\n",
    "\n",
    "data_background = np.load(\"shap_train_final.npz\")\n",
    "x_train = data_background[\"x_background\"]\n",
    "y_train = data_background[\"y_background\"]\n",
    "\n",
    "# Build and load the model\n",
    "model = build_model(hparam)\n",
    "model.load_model('./savepoints/0/model_checkpoint')\n",
    "\n",
    "# Convert data to tensors\n",
    "SS_train, TS_train, GS_train = convert_tensor(x_train, None, SS_mat, TS_mat, GS_mat, mlb, idx2label)\n",
    "\n",
    "# Define feature dimensions\n",
    "SS_shape = SS_train.shape[1]\n",
    "TS_shape = TS_train.shape[1]\n",
    "GS_shape = GS_train.shape[1]\n",
    "\n",
    "# Flatten input tensors for SHAP (concatenate them)\n",
    "X_train_flat = np.concatenate([SS_train, TS_train, GS_train], axis=1)\n",
    "background = X_train_flat  \n",
    "\n",
    "# Define model prediction function for SHAP\n",
    "def model_predict(flattened_input):\n",
    "    # Split the input back into the three components\n",
    "    SS, TS, GS = np.split(flattened_input, [SS_shape, SS_shape + TS_shape], axis=1)\n",
    "\n",
    "    # Convert numpy arrays to PyTorch tensors\n",
    "    SS_tensor = torch.tensor(SS, dtype=torch.float32)\n",
    "    TS_tensor = torch.tensor(TS, dtype=torch.float32)\n",
    "    GS_tensor = torch.tensor(GS, dtype=torch.float32)\n",
    "\n",
    "    # Ensure the model is in evaluation mode\n",
    "    model.eval()\n",
    "\n",
    "    # Perform inference (disable gradient tracking)\n",
    "    with torch.no_grad():\n",
    "        output = model(SS_tensor, TS_tensor, GS_tensor)  # Forward pass through the model\n",
    "\n",
    "    # Convert output to numpy for SHAP compatibility\n",
    "    return output.numpy()\n",
    "\n",
    "# Initialize SHAP explainer\n",
    "explainer = shap.KernelExplainer(model_predict, background)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all batch files\n",
    "import re\n",
    "\n",
    "# Function to extract batch index from filename\n",
    "def sort_key(filename):\n",
    "    match = re.search(r\"batch_(\\d+)\", filename)\n",
    "    return int(match.group(1)) if match else -1\n",
    "\n",
    "# Get and sort batch files numerically\n",
    "batch_files = sorted(glob.glob(\"shap_final_kernel_batch_*.pkl\"), key=sort_key)\n",
    "\n",
    "# Load all batches\n",
    "shap_values_all_batches = [np.array(pickle.load(open(f, \"rb\"))) for f in batch_files]\n",
    "\n",
    "# Sanity check\n",
    "for i, batch in enumerate(shap_values_all_batches):\n",
    "    print(f\"Batch {i}: shape = {batch.shape}\")\n",
    "\n",
    "# Merge along sample axis (axis=1)\n",
    "shap_merged = np.concatenate(shap_values_all_batches, axis=1)\n",
    "print(f\"‚úÖ Merged shape: {shap_merged.shape}\")\n",
    "\n",
    "# Save the merged SHAP values\n",
    "with open(\"shap_final_kernel_merged_ordered.pkl\", \"wb\") as f:\n",
    "    pickle.dump(shap_merged, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def verify_shap_alignment(shap_merged, SS_test, TS_test, GS_test, model, expected_values, label_indices_to_check=None, sample_range=(0, 10)):\n",
    "    model.eval()\n",
    "\n",
    "    errors = []\n",
    "\n",
    "    sample_start, sample_end = sample_range\n",
    "    num_labels = shap_merged.shape[0]\n",
    "\n",
    "    if label_indices_to_check is None:\n",
    "        label_indices_to_check = list(range(num_labels))  # Check all labels by default\n",
    "\n",
    "    for sample_idx in range(sample_start, sample_end):\n",
    "        SS = SS_test[sample_idx:sample_idx+1]\n",
    "        TS = TS_test[sample_idx:sample_idx+1]\n",
    "        GS = GS_test[sample_idx:sample_idx+1]\n",
    "\n",
    "        with torch.no_grad():\n",
    "            logits = model(SS, TS, GS).squeeze(0).numpy()\n",
    "\n",
    "        for label_idx in label_indices_to_check:\n",
    "            shap_sum = shap_merged[label_idx, sample_idx, :].sum()\n",
    "            fx_shap = expected_values[label_idx] + shap_sum\n",
    "            model_logit = logits[label_idx]\n",
    "\n",
    "            if not np.isclose(fx_shap, model_logit, rtol=1e-3, atol=1e-3):\n",
    "                errors.append((sample_idx, label_idx, model_logit, fx_shap))\n",
    "                print(f\"‚ùå MISMATCH [Sample {sample_idx}, Label {label_idx}]: Model={model_logit:.4f}, SHAP-Recon={fx_shap:.4f}\")\n",
    "            else:\n",
    "                print(f\"‚úÖ Match [Sample {sample_idx}, Label {label_idx}]: Model={model_logit:.4f}, SHAP-Recon={fx_shap:.4f}\")\n",
    "\n",
    "    if not errors:\n",
    "        print(\"üéâ All selected samples match!\")\n",
    "    else:\n",
    "        print(f\"\\n‚ö†Ô∏è {len(errors)} mismatches found.\")\n",
    "\n",
    "    return errors\n",
    "\n",
    "expected_values = explainer.expected_value  \n",
    "\n",
    "verify_shap_alignment(\n",
    "    shap_merged,\n",
    "    SS_test,\n",
    "    TS_test,\n",
    "    GS_test,\n",
    "    model,\n",
    "    expected_values,\n",
    "    label_indices_to_check=[73],  # or None for all\n",
    "    sample_range=(0, 47)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# getting the top 10 abs mean shap values for the chosen labels 73, 68, 100 , 43, 104 \n",
    "shap_values_all_batches = shap_merged\n",
    "\n",
    "# Loop through the labels (106 labels)\n",
    "for label_idx in [73, 68, 100 , 43, 104, 99 ]:  \n",
    "    print(f\"Generating combined SHAP plot for label {label_idx}...\")\n",
    "\n",
    "    # Extract SHAP values for this label\n",
    "    shap_values_label = shap_values_all_batches[label_idx]  # Shape: (48, 9582)\n",
    "    \n",
    "    # Assuming you have SS_test, TS_test, and GS_test matrices for features (shape: (30, 3194))\n",
    "    combined_features = np.hstack([SS_test, TS_test, GS_test])  # Shape: (48, 9582)\n",
    "\n",
    "    # Print the shape of combined_features to debug\n",
    "    print(f\"combined_features shape: {combined_features.shape}\")  # Should be (48, 9582)\n",
    "\n",
    "    # Ensure SHAP values and features have matching shapes\n",
    "    if shap_values_label.shape != combined_features.shape:\n",
    "        raise ValueError(\n",
    "            f\"Shape mismatch! SHAP values shape: {shap_values_label.shape}, \"\n",
    "            f\"Feature shape: {combined_features.shape}\"\n",
    "        )\n",
    "        \n",
    "    # Compute global importance (average absolute SHAP value per feature)\n",
    "    mean_shap_values = np.mean(np.abs(shap_values_label), axis=0)  # Shape: (9582,)\n",
    "\n",
    "    # Convert to DataFrame for easy analysis (for each label)\n",
    "    shap_df = pd.DataFrame({\"Feature\": np.arange(9582), \"Mean_SHAP\": mean_shap_values})\n",
    "    shap_df = shap_df.sort_values(by=\"Mean_SHAP\", ascending=False)\n",
    "\n",
    "    print(shap_df.head(10))  # Show top 10 most important features for this label\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# checking the distribution of shap values across all samples and labels\n",
    "\n",
    "# Assuming you already have flattened SHAP values\n",
    "shap_values_flattened = np.concatenate([shap_values.flatten() for shap_values in shap_values_all_batches], axis=0)\n",
    "print(f\"mean: {np.abs(shap_values_flattened).mean()}, min: {shap_values_flattened.min()}, max: {shap_values_flattened.max()}\")\n",
    "\n",
    "# Plot histogram with details on the bin range\n",
    "plt.figure(figsize=(10, 6))\n",
    "n, bins, patches = plt.hist(shap_values_flattened, bins=100, color='skyblue', edgecolor='black', alpha=0.7)\n",
    "\n",
    "# Determine which bin contains the most SHAP values (the aggregated one)\n",
    "max_bin_idx = np.argmax(n)  # Index of the bin with the highest count\n",
    "bin_left = bins[max_bin_idx]  # Left edge of the bin\n",
    "bin_right = bins[max_bin_idx + 1]  # Right edge of the bin\n",
    "bin_count = n[max_bin_idx]  # Number of values in this bin\n",
    "\n",
    "# Print details about the most aggregated bin\n",
    "print(f\"Bin range: ({bin_left}, {bin_right})\")\n",
    "print(f\"Number of SHAP values in this bin: {bin_count}\")\n",
    "print(f\"Total number of SHAP values: {len(shap_values_flattened)}\")\n",
    "print(f\"Percentage of SHAP values in this bin: {(bin_count / len(shap_values_flattened)) * 100:.2f}%\")\n",
    "\n",
    "# Optionally, you can print the SHAP values that fall within this bin range\n",
    "values_in_bin = shap_values_flattened[(shap_values_flattened >= bin_left) & (shap_values_flattened < bin_right)]\n",
    "print(f\"Number of SHAP values in this bin: {len(values_in_bin)}\")\n",
    "print(f\"Sample of SHAP values in this bin: {values_in_bin[:10]}\")  # Show a sample of 10 SHAP values\n",
    "\n",
    "# Optionally, print more detailed statistics of the SHAP values in this bin\n",
    "print(f\"Mean SHAP value in this bin: {np.mean(values_in_bin)}\")\n",
    "print(f\"Standard deviation of SHAP values in this bin: {np.std(values_in_bin)}\")\n",
    "\n",
    "# Show the plot with the most populated bin highlighted\n",
    "plt.title(\"Distribution of SHAP values with aggregated bin\", fontsize=14)\n",
    "plt.xlabel(\"SHAP Value\", fontsize=12)\n",
    "plt.ylabel(\"Frequency\", fontsize=12)\n",
    "plt.axvline(bin_left, color='red', linestyle='dashed', linewidth=2)\n",
    "plt.axvline(bin_right, color='red', linestyle='dashed', linewidth=2)\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mapping of drug features \n",
    "\n",
    "import joblib\n",
    "\n",
    "# Load the drug name to index mapping\n",
    "with open('./data/drugName2idx.pkl', 'rb') as f:\n",
    "    drugName2idx = joblib.load(f)\n",
    "\n",
    "def map_feature_to_drug_pair_and_matrix(feature_idx, matrix_size=1597):\n",
    "    # Calculate which drug pair and matrix type this feature corresponds to\n",
    "    drug_idx = feature_idx % 1597  # Modulo to get index of drug in pair (0-1596)\n",
    "\n",
    "    # Determine which matrix the feature belongs to\n",
    "    if feature_idx < matrix_size:\n",
    "        matrix_type = \"SS\"  # Structural similarity for drug A\n",
    "        drug_a_name = list(drugName2idx.keys())[drug_idx]  # Fetch Drug A's similar drug from SS matrix\n",
    "        drug_b_name = None\n",
    "    elif feature_idx < 2 * matrix_size:\n",
    "        matrix_type = \"SS\"  # Structural similarity for drug B\n",
    "        drug_a_name = None\n",
    "        drug_b_name = list(drugName2idx.keys())[drug_idx]  # Fetch Drug B's similar drug from SS matrix\n",
    "    elif feature_idx < 3 * matrix_size:\n",
    "        matrix_type = \"TS\"  # Target similarity for drug A\n",
    "        drug_a_name = list(drugName2idx.keys())[drug_idx]  # Fetch Drug A's similar drug from TS matrix\n",
    "        drug_b_name = None\n",
    "    elif feature_idx < 4 * matrix_size:\n",
    "        matrix_type = \"TS\"  # Target similarity for drug B\n",
    "        drug_a_name = None\n",
    "        drug_b_name = list(drugName2idx.keys())[drug_idx]  # Fetch Drug B's similar drug from TS matrix\n",
    "    elif feature_idx < 5 * matrix_size:\n",
    "        matrix_type = \"GS\"  # GO similarity for drug A\n",
    "        drug_a_name = list(drugName2idx.keys())[drug_idx]  # Fetch Drug A's similar drug from GS matrix\n",
    "        drug_b_name = None\n",
    "    else:\n",
    "        matrix_type = \"GS\"  # GO similarity for drug B\n",
    "        drug_a_name = None\n",
    "        drug_b_name = list(drugName2idx.keys())[drug_idx]  # Fetch Drug B's similar drug from TS matrix\n",
    "\n",
    "    return drug_a_name, drug_b_name, matrix_type\n",
    "\n",
    "feature_names = []\n",
    "\n",
    "for feature_idx in range(9582):  # 9582 features (drug pairs)\n",
    "    # Map feature to drug pair and matrix type\n",
    "    drug_a, drug_b, matrix_type = map_feature_to_drug_pair_and_matrix(feature_idx)\n",
    "        \n",
    "    # Generate a descriptive feature name for this feature\n",
    "    if drug_a:\n",
    "        feature_name = f\"Drug A similarity to {drug_a} - {matrix_type}\"\n",
    "    else:\n",
    "        feature_name = f\"Drug B similarity to {drug_b} - {matrix_type}\"\n",
    "\n",
    "    feature_names.append(feature_name)\n",
    "\n",
    "# Print out lengths for debugging\n",
    "print(f\"Length of feature_names: {len(feature_names)}\")  # Should be 9582\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generating beeswarm plots for all labels sorted according to (max) shap values\n",
    "\n",
    "# Iterate over the labels and features to print and visualize the SHAP values\n",
    "for label_idx in [73, 68, 100 , 43, 104, 99 ]:  \n",
    "    print(f\"SHAP values for label {label_idx}\")\n",
    "    \n",
    "    # Step 1: Stack your model inputs to form the full feature matrix (samples x features)\n",
    "    flat_input = np.concatenate([SS_test, TS_test, GS_test], axis=1)  # Shape: (num_samples, 9582)\n",
    "\n",
    "    # Step 2: SHAP values for this label across all samples\n",
    "    shap_values_matrix = shap_values_all_batches[label_idx]  # shape: (num_samples, 9582)\n",
    "    # Print out the shape of shap_values_matrix for debugging\n",
    "   # print(f\"Shape of shap_values_matrix for label {label_idx}: {shap_values_matrix.shape}\")  # Should be (num_samples, 9582)\n",
    "\n",
    "\n",
    "    # Step 3: Get top feature indices by max(abs(shap)) across samples\n",
    "    top_n = 10\n",
    "    top_feature_indices = np.argsort(np.abs(shap_values_matrix).mean(axis=0))[::-1][:top_n]\n",
    "    top_feature_names = [feature_names[i] for i in top_feature_indices]\n",
    "\n",
    "    # Step 4: Construct SHAP Explanation object with top features only\n",
    "    shap_exp = shap.Explanation(\n",
    "    values=shap_values_matrix[:, top_feature_indices],           # (num_samples, top_n)\n",
    "    data=flat_input[:, top_feature_indices],                     # (num_samples, top_n)\n",
    "    feature_names= top_feature_names\n",
    "    )\n",
    "\n",
    "    # Step 5: Plot beeswarm\n",
    "    shap.plots.beeswarm(shap_exp, max_display=top_n)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generating beeswarm plots for all labels sorted according to (mean) shap values\n",
    "\n",
    "# Iterate over the labels and features to print and visualize the SHAP values\n",
    "for label_idx in [73, 68, 100 , 43, 104, 99 ]:  \n",
    "    print(f\"SHAP values for label {label_idx}...\")\n",
    "    \n",
    "    # Step 1: Stack your model inputs to form the full feature matrix (samples x features)\n",
    "    flat_input = np.concatenate([SS_test, TS_test, GS_test], axis=1)  # Shape: (num_samples, 9582)\n",
    "\n",
    "    # Step 2: SHAP values for this label across all samples\n",
    "    shap_values_matrix = shap_values_all_batches[label_idx]  # shape: (num_samples, 9582)\n",
    "    # Print out the shape of shap_values_matrix for debugging\n",
    "    print(f\"Shape of shap_values_matrix for label {label_idx}: {shap_values_matrix.shape}\")  # Should be (num_samples, 9582)\n",
    "\n",
    "\n",
    "    # Step 3: Get top feature indices by mean(abs(shap)) across samples\n",
    "    top_n = 15\n",
    "    top_feature_indices = np.argsort(np.abs(shap_values_matrix).mean(axis=0))[::-1][:top_n]\n",
    "    top_feature_names = [feature_names[i] for i in top_feature_indices]\n",
    "\n",
    "    # Step 4: Construct SHAP Explanation object with top features only\n",
    "    shap_exp = shap.Explanation(\n",
    "    values=shap_values_matrix[:, top_feature_indices],           # (num_samples, top_n)\n",
    "    data=flat_input[:, top_feature_indices],                     # (num_samples, top_n)\n",
    "    feature_names= top_feature_names\n",
    "    )\n",
    "\n",
    "    # Step 5: Plot beeswarm\n",
    "    #shap.plots.beeswarm(shap_exp, max_display=top_n)\n",
    "    shap.plots.beeswarm(shap_exp.abs, color= 'shap_red', max_display=top_n)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generating bar plots to show top 10 abs mean SHAP values\n",
    "for label_idx in [73, 68, 100, 43, 104, 99]:\n",
    "    print(f\"Processing SHAP values for label {label_idx}...\")\n",
    "\n",
    "    # Compute mean absolute SHAP values\n",
    "    abs_mean_shap_values = np.mean(np.abs(shap_values_all_batches[label_idx]), axis=0)\n",
    "\n",
    "    # Sort features by importance (descending)\n",
    "    sorted_indices = np.argsort(abs_mean_shap_values)[::-1]\n",
    "    top_n = 10  # Number of features to display\n",
    "\n",
    "    # Get top feature names and values\n",
    "    top_features = [feature_names[i] for i in sorted_indices[:top_n]]\n",
    "    top_values = abs_mean_shap_values[sorted_indices[:top_n]]\n",
    "\n",
    "    # Create horizontal bar plot\n",
    "    plt.figure(figsize=(14, 10))\n",
    "    bars = plt.barh(top_features[::-1], top_values[::-1], color='royalblue')\n",
    "\n",
    "    # Get the maximum value for annotations\n",
    "    max_value = max(top_values)\n",
    "\n",
    "    # Annotate each bar with its value\n",
    "    for bar in bars:\n",
    "        width = bar.get_width()\n",
    "        plt.text(\n",
    "            width + 0.01 * max_value,  # Position text slightly right of the bar\n",
    "            bar.get_y() + bar.get_height() / 2,  # Center text vertically\n",
    "            f\"{width:.3f}\",  # Format SHAP value to 4 decimal places\n",
    "            ha='left',\n",
    "            va='center',\n",
    "            fontsize=10\n",
    "        )\n",
    "\n",
    "    # Add labels and title\n",
    "    plt.xlabel(\"Mean |SHAP value|\", fontsize=12)\n",
    "    plt.ylabel(\"Feature\", fontsize=12)\n",
    "    plt.title(f\"Top {top_n} Features for Label {label_idx}\", fontsize=14)\n",
    "    #plt.tight_layout()  # Prevent label cutoff\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# summary plots for the predicted label shap values \n",
    "\n",
    "# Define how many samples to analyze\n",
    "num_samples = 48\n",
    "\n",
    "# Store SHAP values for each sample\n",
    "shap_values_pred_label_all = []\n",
    "\n",
    "# Loop over samples\n",
    "for sample_idx in range(num_samples):\n",
    "    print(f\"Analyzing sample {sample_idx + 1}...\")\n",
    "\n",
    "    # Extract input sample (SS, TS, GS matrices)\n",
    "    SS_sample = SS_test[sample_idx:sample_idx+1].clone().detach()\n",
    "    TS_sample = TS_test[sample_idx:sample_idx+1].clone().detach()\n",
    "    GS_sample = GS_test[sample_idx:sample_idx+1].clone().detach()\n",
    "\n",
    "    # Ensure model is in evaluation mode\n",
    "    model.eval()\n",
    "\n",
    "    # Step 1: Get model predictions for this sample\n",
    "    with torch.no_grad():\n",
    "        predictions = model(SS_sample, TS_sample, GS_sample)\n",
    "\n",
    "    # Convert predictions to NumPy array\n",
    "    predictions_np = predictions.clone().detach().numpy()\n",
    "\n",
    "    # Step 2: Find the predicted label index\n",
    "    predicted_label_idx = np.argmax(predictions_np)\n",
    "    print(f'Predicted label index for sample {sample_idx}: {predicted_label_idx}')\n",
    "\n",
    "    # Step 3: Extract SHAP values for the predicted label\n",
    "    shap_values_pred_label = shap_values_all_batches[predicted_label_idx, sample_idx, :]\n",
    "\n",
    "    # Store for later aggregation\n",
    "    shap_values_pred_label_all.append(shap_values_pred_label)\n",
    "\n",
    "# Convert list to NumPy array (shape: num_samples √ó num_features)\n",
    "shap_values_pred_label_all = np.array(shap_values_pred_label_all)\n",
    "\n",
    "# Step 4: Identify Top `N` Features (Based on Mean Absolute SHAP Value)\n",
    "top_n = 10\n",
    "mean_abs_shap = np.mean(np.abs(shap_values_pred_label_all), axis=0)  # Mean SHAP across samples\n",
    "top_feature_indices = np.argsort(mean_abs_shap)[::-1][:top_n]  # Get indices of top features\n",
    "print(top_feature_indices)\n",
    "\n",
    "# Step 5: Extract Corresponding SHAP Values & Feature Names\n",
    "top_shap_values = shap_values_pred_label_all[:, top_feature_indices]  # SHAP values for top features\n",
    "top_feature_names = [feature_names[i] for i in top_feature_indices]  # Get feature names\n",
    "\n",
    "# Step 6: Generate SHAP Plots\n",
    "print(\"Generating SHAP summary plots...\")\n",
    "\n",
    "# Summary Plot (Beeswarm)\n",
    "shap.summary_plot(shap_values_pred_label_all, feature_names=feature_names, max_display=15)\n",
    "\n",
    "# Bar Plot (Feature Importance)\n",
    "shap.summary_plot(shap_values_pred_label_all, feature_names=feature_names, plot_type=\"bar\", max_display=15)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loop over samples and generate waterfall plots for the predicted label showing the probability of the prediction and feature values\n",
    "\n",
    "# Define how many samples to loop through \n",
    "num_samples = 48\n",
    "\n",
    "# Invert the drugName2idx mapping to get drug names from indices\n",
    "idx2drug = {v: k for k, v in drugName2idx.items()}\n",
    "\n",
    "# Loop over the samples\n",
    "for sample_idx in range(num_samples):\n",
    "    print(f\"Analyzing sample {sample_idx}...\")\n",
    "\n",
    "    # Extract the current sample and true label\n",
    "    x_instance = x_test[sample_idx] \n",
    "    y_instance = y_test[sample_idx]\n",
    "    print('Drug pair indices', x_instance)\n",
    "    print('y_instance',y_instance)\n",
    "\n",
    "    # Get one label vector and convert it for inverse_transform\n",
    "    y_instance = y_test[sample_idx].cpu().numpy().reshape(1, -1)  # Convert to shape (1, 106)\n",
    "\n",
    "    # Convert multi-hot back to original labels\n",
    "    true_labels = mlb.inverse_transform(y_instance)[0]\n",
    "\n",
    "    # Extract the drug names from drugName2idx by indexing the dictionary\n",
    "    drug_names = [idx2drug[idx] for idx in x_instance]\n",
    "\n",
    "    # Print the drug names\n",
    "    print(\"Drug names for x_instance:\", drug_names)\n",
    "    \n",
    "    print(\"True Labels:\", true_labels)\n",
    "\n",
    "    # Extract sample data for SHAP (SS, TS, GS)\n",
    "    SS_sample = SS_test[sample_idx:sample_idx+1].clone().detach()\n",
    "    TS_sample = TS_test[sample_idx:sample_idx+1].clone().detach()\n",
    "    GS_sample = GS_test[sample_idx:sample_idx+1].clone().detach()\n",
    "    feature_input_values = np.concatenate([SS_sample, TS_sample, GS_sample], axis=1)\n",
    "  \n",
    "    # Ensure the model is in evaluation mode\n",
    "    model.eval()\n",
    "\n",
    "    # Step 1: Get model predictions for this sample\n",
    "    with torch.no_grad():\n",
    "        # Get raw model predictions (logits)\n",
    "        raw_logits = model(SS_sample, TS_sample, GS_sample)\n",
    "\n",
    "        # Convert logits to probabilities (optional, for thresholding or display)\n",
    "        probabilities = torch.sigmoid(raw_logits)  # For binary classification\n",
    "\n",
    "        # If you need predictions (thresholding at 0.5)\n",
    "        predictions = (probabilities > 0.5).int()\n",
    "    # Convert to NumPy\n",
    "    predictions_np = predictions.clone().detach().numpy()\n",
    "\n",
    "    # Step 2: Find the predicted label (highest probability)\n",
    "    predicted_label_idx = np.argmax(predictions_np)\n",
    "    print('Predicted label index:', predicted_label_idx)\n",
    "\n",
    "    # Step 3: Extract SHAP values for the predicted label\n",
    "    shap_values_pred_label = shap_values_all_batches[predicted_label_idx, sample_idx, :]\n",
    "\n",
    "    # Step 4: Create SHAP Explanation object\n",
    "\n",
    "    shap_explanation = shap.Explanation(\n",
    "    values=shap_values_pred_label,  # All values (not top 10)\n",
    "    base_values=explainer.expected_value[predicted_label_idx],\n",
    "    data=feature_input_values[0],\n",
    "    feature_names=feature_names\n",
    "    )\n",
    "   \n",
    "    # Step 5: Plot Waterfall Chart for Predicted Label\n",
    "\n",
    "    # Get f(x) = logit and P = sigmoid(f(x))\n",
    "    \n",
    "    print(\"SHAP values sum for the sample :\", shap_values_pred_label.sum())\n",
    "    \n",
    "    # Check if SHAP values sum to model output (logit)\n",
    "    fx_shap = explainer.expected_value[predicted_label_idx] + shap_values_pred_label.sum()\n",
    "    print(\"Expected value:\", explainer.expected_value[predicted_label_idx])\n",
    "\n",
    "    # Get raw model logit (avoid probability conversion)\n",
    "    logit_fx = raw_logits[0, predicted_label_idx].item()\n",
    "    \n",
    "    print(f\"Model logit: {logit_fx:.4f} | SHAP-reconstructed logit: {fx_shap:.4f}\")\n",
    "    \n",
    "    probability = torch.sigmoid(raw_logits)[0, predicted_label_idx].item()\n",
    "    print(f\"Prediction Probability:{probability:.4f}\")\n",
    "    \n",
    "    print(\"Model output (logits):\", raw_logits)\n",
    "    print(\"Sigmoid(logits):\", torch.sigmoid(raw_logits))\n",
    "\n",
    "    shap.plots.waterfall(shap_explanation, max_display = 10, show=False)\n",
    "    plt.title(f\"SHAP Waterfall Plot for Sample {sample_idx} (Probability = {probability:.4f})\", fontsize=14)\n",
    "    plt.figure(figsize=(10, 6)) \n",
    "    plt.show()\n",
    "    print(\"\\n\" + \"=\"*50 + \"\\n\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
